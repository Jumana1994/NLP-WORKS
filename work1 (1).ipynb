{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df1d2e9-607b-44eb-b558-1cd837893773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dccff1d-2483-4200-9ac6-78da89e6150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"Reading has been proven to strengthen our brain. Reading is a process in which our brain is trained to keep new information and new topics. It also helps with writing and speaking fluentlyReading has also proven to increase our empathy. Literary fiction — stories that explore the inner lives of characters — are especially beneficial for growing people’s empathy towards others.Reading builds vocabulary which is very important for people. Vocabulary size can influence many areas of your life, from scores on tests to college admissions and job opportunities. Reading books is the best way to enlarge your exposure to new words.Reading also releases stress, slows down, age-related cognitive decline, cures depression symptoms and therefore provides us ample benefits.Good books can also help inspire other people to write. Research also proposes that children who read for entertainment every day not only achieve more in reading tests than those who do not but they also exhibit a better vocabulary, increased general knowledge and a better perception of different cultures.Also, reading for enjoyment is more likely to decide whether a child does well at school than their social or financial background.If you are looking for custom essays on reading topics, there is the fastest and reliable way to buy essay online from academic writing experts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48d4c36-51bf-4842-b5e1-1f40a7aeaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f06081-e6b8-4fc1-97e3-ae03704e0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbd17cf-d459-4a4f-9846-f7c098b6ede0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bb4802-e1ae-4a3a-8a13-db654ad531e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "frequency=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c9172a-a539-4f7b-9aa5-b2df0ced896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in word:\n",
    "    frequency[i.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dc8b2c-8023-4bbf-8638-c217691a4586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'to': 9, ',': 7, 'reading': 6, '.': 6, 'is': 6, 'and': 6, 'also': 6, 'for': 5, 'a': 4, 'our': 3, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc5ca5ca-5cad-4d1a-a1a6-21f357491cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aafe05-c39b-48b3-8aa7-630a707c15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in word:\n",
    "    if i not in string.punctuation:\n",
    "        a.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31ba875f-e65c-471c-9fb0-9c688300487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f37ef45-d9be-48aa-a498-b4a522f960e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c6bf4f-04e9-4808-8a41-1c83b82573b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - read\n",
      "has - ha\n",
      "been - been\n",
      "proven - proven\n",
      "to - to\n",
      "strengthen - strengthen\n",
      "our - our\n",
      "brain - brain\n",
      "Reading - read\n",
      "is - is\n",
      "a - a\n",
      "process - process\n",
      "in - in\n",
      "which - which\n",
      "our - our\n",
      "brain - brain\n",
      "is - is\n",
      "trained - train\n",
      "to - to\n",
      "keep - keep\n",
      "new - new\n",
      "information - inform\n",
      "and - and\n",
      "new - new\n",
      "topics - topic\n",
      "It - it\n",
      "also - also\n",
      "helps - help\n",
      "with - with\n",
      "writing - write\n",
      "and - and\n",
      "speaking - speak\n",
      "fluentlyReading - fluentlyread\n",
      "has - ha\n",
      "also - also\n",
      "proven - proven\n",
      "to - to\n",
      "increase - increas\n",
      "our - our\n",
      "empathy - empathi\n",
      "Literary - literari\n",
      "fiction - fiction\n",
      "— - —\n",
      "stories - stori\n",
      "that - that\n",
      "explore - explor\n",
      "the - the\n",
      "inner - inner\n",
      "lives - live\n",
      "of - of\n",
      "characters - charact\n",
      "— - —\n",
      "are - are\n",
      "especially - especi\n",
      "beneficial - benefici\n",
      "for - for\n",
      "growing - grow\n",
      "people - peopl\n",
      "’ - ’\n",
      "s - s\n",
      "empathy - empathi\n",
      "towards - toward\n",
      "others.Reading - others.read\n",
      "builds - build\n",
      "vocabulary - vocabulari\n",
      "which - which\n",
      "is - is\n",
      "very - veri\n",
      "important - import\n",
      "for - for\n",
      "people - peopl\n",
      "Vocabulary - vocabulari\n",
      "size - size\n",
      "can - can\n",
      "influence - influenc\n",
      "many - mani\n",
      "areas - area\n",
      "of - of\n",
      "your - your\n",
      "life - life\n",
      "from - from\n",
      "scores - score\n",
      "on - on\n",
      "tests - test\n",
      "to - to\n",
      "college - colleg\n",
      "admissions - admiss\n",
      "and - and\n",
      "job - job\n",
      "opportunities - opportun\n",
      "Reading - read\n",
      "books - book\n",
      "is - is\n",
      "the - the\n",
      "best - best\n",
      "way - way\n",
      "to - to\n",
      "enlarge - enlarg\n",
      "your - your\n",
      "exposure - exposur\n",
      "to - to\n",
      "new - new\n",
      "words.Reading - words.read\n",
      "also - also\n",
      "releases - releas\n",
      "stress - stress\n",
      "slows - slow\n",
      "down - down\n",
      "age-related - age-rel\n",
      "cognitive - cognit\n",
      "decline - declin\n",
      "cures - cure\n",
      "depression - depress\n",
      "symptoms - symptom\n",
      "and - and\n",
      "therefore - therefor\n",
      "provides - provid\n",
      "us - us\n",
      "ample - ampl\n",
      "benefits.Good - benefits.good\n",
      "books - book\n",
      "can - can\n",
      "also - also\n",
      "help - help\n",
      "inspire - inspir\n",
      "other - other\n",
      "people - peopl\n",
      "to - to\n",
      "write - write\n",
      "Research - research\n",
      "also - also\n",
      "proposes - propos\n",
      "that - that\n",
      "children - children\n",
      "who - who\n",
      "read - read\n",
      "for - for\n",
      "entertainment - entertain\n",
      "every - everi\n",
      "day - day\n",
      "not - not\n",
      "only - onli\n",
      "achieve - achiev\n",
      "more - more\n",
      "in - in\n",
      "reading - read\n",
      "tests - test\n",
      "than - than\n",
      "those - those\n",
      "who - who\n",
      "do - do\n",
      "not - not\n",
      "but - but\n",
      "they - they\n",
      "also - also\n",
      "exhibit - exhibit\n",
      "a - a\n",
      "better - better\n",
      "vocabulary - vocabulari\n",
      "increased - increas\n",
      "general - gener\n",
      "knowledge - knowledg\n",
      "and - and\n",
      "a - a\n",
      "better - better\n",
      "perception - percept\n",
      "of - of\n",
      "different - differ\n",
      "cultures.Also - cultures.also\n",
      "reading - read\n",
      "for - for\n",
      "enjoyment - enjoy\n",
      "is - is\n",
      "more - more\n",
      "likely - like\n",
      "to - to\n",
      "decide - decid\n",
      "whether - whether\n",
      "a - a\n",
      "child - child\n",
      "does - doe\n",
      "well - well\n",
      "at - at\n",
      "school - school\n",
      "than - than\n",
      "their - their\n",
      "social - social\n",
      "or - or\n",
      "financial - financi\n",
      "background.If - background.if\n",
      "you - you\n",
      "are - are\n",
      "looking - look\n",
      "for - for\n",
      "custom - custom\n",
      "essays - essay\n",
      "on - on\n",
      "reading - read\n",
      "topics - topic\n",
      "there - there\n",
      "is - is\n",
      "the - the\n",
      "fastest - fastest\n",
      "and - and\n",
      "reliable - reliabl\n",
      "way - way\n",
      "to - to\n",
      "buy - buy\n",
      "essay - essay\n",
      "online - onlin\n",
      "from - from\n",
      "academic - academ\n",
      "writing - write\n",
      "experts - expert\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i,'-',stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f00eb8-4cb7-4682-97fc-afdc41fb0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44877bf-cb2d-4209-914e-3575a47032b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec9ea2c-9191-4c4e-ab19-376bdbbcb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - Reading\n",
      "has - ha\n",
      "been - been\n",
      "proven - proven\n",
      "to - to\n",
      "strengthen - strengthen\n",
      "our - our\n",
      "brain - brain\n",
      "Reading - Reading\n",
      "is - is\n",
      "a - a\n",
      "process - process\n",
      "in - in\n",
      "which - which\n",
      "our - our\n",
      "brain - brain\n",
      "is - is\n",
      "trained - trained\n",
      "to - to\n",
      "keep - keep\n",
      "new - new\n",
      "information - information\n",
      "and - and\n",
      "new - new\n",
      "topics - topic\n",
      "It - It\n",
      "also - also\n",
      "helps - help\n",
      "with - with\n",
      "writing - writing\n",
      "and - and\n",
      "speaking - speaking\n",
      "fluentlyReading - fluentlyReading\n",
      "has - ha\n",
      "also - also\n",
      "proven - proven\n",
      "to - to\n",
      "increase - increase\n",
      "our - our\n",
      "empathy - empathy\n",
      "Literary - Literary\n",
      "fiction - fiction\n",
      "— - —\n",
      "stories - story\n",
      "that - that\n",
      "explore - explore\n",
      "the - the\n",
      "inner - inner\n",
      "lives - life\n",
      "of - of\n",
      "characters - character\n",
      "— - —\n",
      "are - are\n",
      "especially - especially\n",
      "beneficial - beneficial\n",
      "for - for\n",
      "growing - growing\n",
      "people - people\n",
      "’ - ’\n",
      "s - s\n",
      "empathy - empathy\n",
      "towards - towards\n",
      "others.Reading - others.Reading\n",
      "builds - build\n",
      "vocabulary - vocabulary\n",
      "which - which\n",
      "is - is\n",
      "very - very\n",
      "important - important\n",
      "for - for\n",
      "people - people\n",
      "Vocabulary - Vocabulary\n",
      "size - size\n",
      "can - can\n",
      "influence - influence\n",
      "many - many\n",
      "areas - area\n",
      "of - of\n",
      "your - your\n",
      "life - life\n",
      "from - from\n",
      "scores - score\n",
      "on - on\n",
      "tests - test\n",
      "to - to\n",
      "college - college\n",
      "admissions - admission\n",
      "and - and\n",
      "job - job\n",
      "opportunities - opportunity\n",
      "Reading - Reading\n",
      "books - book\n",
      "is - is\n",
      "the - the\n",
      "best - best\n",
      "way - way\n",
      "to - to\n",
      "enlarge - enlarge\n",
      "your - your\n",
      "exposure - exposure\n",
      "to - to\n",
      "new - new\n",
      "words.Reading - words.Reading\n",
      "also - also\n",
      "releases - release\n",
      "stress - stress\n",
      "slows - slows\n",
      "down - down\n",
      "age-related - age-related\n",
      "cognitive - cognitive\n",
      "decline - decline\n",
      "cures - cure\n",
      "depression - depression\n",
      "symptoms - symptom\n",
      "and - and\n",
      "therefore - therefore\n",
      "provides - provides\n",
      "us - u\n",
      "ample - ample\n",
      "benefits.Good - benefits.Good\n",
      "books - book\n",
      "can - can\n",
      "also - also\n",
      "help - help\n",
      "inspire - inspire\n",
      "other - other\n",
      "people - people\n",
      "to - to\n",
      "write - write\n",
      "Research - Research\n",
      "also - also\n",
      "proposes - proposes\n",
      "that - that\n",
      "children - child\n",
      "who - who\n",
      "read - read\n",
      "for - for\n",
      "entertainment - entertainment\n",
      "every - every\n",
      "day - day\n",
      "not - not\n",
      "only - only\n",
      "achieve - achieve\n",
      "more - more\n",
      "in - in\n",
      "reading - reading\n",
      "tests - test\n",
      "than - than\n",
      "those - those\n",
      "who - who\n",
      "do - do\n",
      "not - not\n",
      "but - but\n",
      "they - they\n",
      "also - also\n",
      "exhibit - exhibit\n",
      "a - a\n",
      "better - better\n",
      "vocabulary - vocabulary\n",
      "increased - increased\n",
      "general - general\n",
      "knowledge - knowledge\n",
      "and - and\n",
      "a - a\n",
      "better - better\n",
      "perception - perception\n",
      "of - of\n",
      "different - different\n",
      "cultures.Also - cultures.Also\n",
      "reading - reading\n",
      "for - for\n",
      "enjoyment - enjoyment\n",
      "is - is\n",
      "more - more\n",
      "likely - likely\n",
      "to - to\n",
      "decide - decide\n",
      "whether - whether\n",
      "a - a\n",
      "child - child\n",
      "does - doe\n",
      "well - well\n",
      "at - at\n",
      "school - school\n",
      "than - than\n",
      "their - their\n",
      "social - social\n",
      "or - or\n",
      "financial - financial\n",
      "background.If - background.If\n",
      "you - you\n",
      "are - are\n",
      "looking - looking\n",
      "for - for\n",
      "custom - custom\n",
      "essays - essay\n",
      "on - on\n",
      "reading - reading\n",
      "topics - topic\n",
      "there - there\n",
      "is - is\n",
      "the - the\n",
      "fastest - fastest\n",
      "and - and\n",
      "reliable - reliable\n",
      "way - way\n",
      "to - to\n",
      "buy - buy\n",
      "essay - essay\n",
      "online - online\n",
      "from - from\n",
      "academic - academic\n",
      "writing - writing\n",
      "experts - expert\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i,'-',lemme.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f931e21-93e3-4825-b131-6cd47a679b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "723c160d-2317-4819-8850-e0174f783692",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in a:\n",
    "    if i not in stop_word:\n",
    "        b.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a73925e3-f915-4dfe-ace6-760a2adad7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Reading', 'VBG'),\n",
       " ('proven', 'RB'),\n",
       " ('strengthen', 'VB'),\n",
       " ('brain', 'NN'),\n",
       " ('Reading', 'NNP'),\n",
       " ('process', 'NN'),\n",
       " ('brain', 'NN'),\n",
       " ('trained', 'VBD'),\n",
       " ('keep', 'VB'),\n",
       " ('new', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('new', 'JJ'),\n",
       " ('topics', 'NNS'),\n",
       " ('It', 'PRP'),\n",
       " ('also', 'RB'),\n",
       " ('helps', 'VBZ'),\n",
       " ('writing', 'VBG'),\n",
       " ('speaking', 'VBG'),\n",
       " ('fluentlyReading', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('proven', 'JJ'),\n",
       " ('increase', 'NN'),\n",
       " ('empathy', 'JJ'),\n",
       " ('Literary', 'NNP'),\n",
       " ('fiction', 'NN'),\n",
       " ('—', 'NNP'),\n",
       " ('stories', 'NNS'),\n",
       " ('explore', 'VBP'),\n",
       " ('inner', 'JJ'),\n",
       " ('lives', 'NNS'),\n",
       " ('characters', 'NNS'),\n",
       " ('—', 'VBP'),\n",
       " ('especially', 'RB'),\n",
       " ('beneficial', 'JJ'),\n",
       " ('growing', 'VBG'),\n",
       " ('people', 'NNS'),\n",
       " ('’', 'NNP'),\n",
       " ('empathy', 'JJ'),\n",
       " ('towards', 'NNS'),\n",
       " ('others.Reading', 'VBG'),\n",
       " ('builds', 'NNS'),\n",
       " ('vocabulary', 'JJ'),\n",
       " ('important', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('Vocabulary', 'NNP'),\n",
       " ('size', 'NN'),\n",
       " ('influence', 'NN'),\n",
       " ('many', 'JJ'),\n",
       " ('areas', 'NNS'),\n",
       " ('life', 'NN'),\n",
       " ('scores', 'NNS'),\n",
       " ('tests', 'VBZ'),\n",
       " ('college', 'NN'),\n",
       " ('admissions', 'NNS'),\n",
       " ('job', 'NN'),\n",
       " ('opportunities', 'NNS'),\n",
       " ('Reading', 'NNP'),\n",
       " ('books', 'NNS'),\n",
       " ('best', 'JJS'),\n",
       " ('way', 'NN'),\n",
       " ('enlarge', 'NN'),\n",
       " ('exposure', 'VBP'),\n",
       " ('new', 'JJ'),\n",
       " ('words.Reading', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('releases', 'VBZ'),\n",
       " ('stress', 'JJ'),\n",
       " ('slows', 'VBZ'),\n",
       " ('age-related', 'JJ'),\n",
       " ('cognitive', 'JJ'),\n",
       " ('decline', 'NN'),\n",
       " ('cures', 'NNS'),\n",
       " ('depression', 'NN'),\n",
       " ('symptoms', 'NNS'),\n",
       " ('therefore', 'RB'),\n",
       " ('provides', 'VBZ'),\n",
       " ('us', 'PRP'),\n",
       " ('ample', 'JJ'),\n",
       " ('benefits.Good', 'NN'),\n",
       " ('books', 'NNS'),\n",
       " ('also', 'RB'),\n",
       " ('help', 'VBP'),\n",
       " ('inspire', 'VB'),\n",
       " ('people', 'NNS'),\n",
       " ('write', 'VBP'),\n",
       " ('Research', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('proposes', 'VBZ'),\n",
       " ('children', 'NNS'),\n",
       " ('read', 'JJ'),\n",
       " ('entertainment', 'NN'),\n",
       " ('every', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('achieve', 'VB'),\n",
       " ('reading', 'NN'),\n",
       " ('tests', 'NNS'),\n",
       " ('also', 'RB'),\n",
       " ('exhibit', 'VBP'),\n",
       " ('better', 'JJR'),\n",
       " ('vocabulary', 'NN'),\n",
       " ('increased', 'VBD'),\n",
       " ('general', 'JJ'),\n",
       " ('knowledge', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " ('perception', 'NN'),\n",
       " ('different', 'JJ'),\n",
       " ('cultures.Also', 'NN'),\n",
       " ('reading', 'NN'),\n",
       " ('enjoyment', 'NN'),\n",
       " ('likely', 'RB'),\n",
       " ('decide', 'IN'),\n",
       " ('whether', 'IN'),\n",
       " ('child', 'NN'),\n",
       " ('well', 'RB'),\n",
       " ('school', 'NN'),\n",
       " ('social', 'JJ'),\n",
       " ('financial', 'JJ'),\n",
       " ('background.If', 'NN'),\n",
       " ('looking', 'VBG'),\n",
       " ('custom', 'NN'),\n",
       " ('essays', 'NNS'),\n",
       " ('reading', 'VBG'),\n",
       " ('topics', 'NNS'),\n",
       " ('fastest', 'JJS'),\n",
       " ('reliable', 'JJ'),\n",
       " ('way', 'NN'),\n",
       " ('buy', 'VB'),\n",
       " ('essay', 'JJ'),\n",
       " ('online', 'JJ'),\n",
       " ('academic', 'JJ'),\n",
       " ('writing', 'NN'),\n",
       " ('experts', 'NNS')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_string1=nltk.pos_tag(b)\n",
    "pos_string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "163140c1-20cf-4c04-8603-bd2e55007e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8af49f4f-443b-4e5c-9135-406668b722c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner1=ne_chunk(pos_string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f595bafd-71b7-4f54-b1a1-a923025a56f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Reading/VBG\n",
      "  proven/RB\n",
      "  strengthen/VB\n",
      "  brain/NN\n",
      "  Reading/NNP\n",
      "  process/NN\n",
      "  brain/NN\n",
      "  trained/VBD\n",
      "  keep/VB\n",
      "  new/JJ\n",
      "  information/NN\n",
      "  new/JJ\n",
      "  topics/NNS\n",
      "  It/PRP\n",
      "  also/RB\n",
      "  helps/VBZ\n",
      "  writing/VBG\n",
      "  speaking/VBG\n",
      "  (ORGANIZATION fluentlyReading/NN)\n",
      "  also/RB\n",
      "  proven/JJ\n",
      "  increase/NN\n",
      "  empathy/JJ\n",
      "  Literary/NNP\n",
      "  fiction/NN\n",
      "  —/NNP\n",
      "  stories/NNS\n",
      "  explore/VBP\n",
      "  inner/JJ\n",
      "  lives/NNS\n",
      "  characters/NNS\n",
      "  —/VBP\n",
      "  especially/RB\n",
      "  beneficial/JJ\n",
      "  growing/VBG\n",
      "  people/NNS\n",
      "  ’/NNP\n",
      "  empathy/JJ\n",
      "  towards/NNS\n",
      "  others.Reading/VBG\n",
      "  builds/NNS\n",
      "  vocabulary/JJ\n",
      "  important/JJ\n",
      "  people/NNS\n",
      "  Vocabulary/NNP\n",
      "  size/NN\n",
      "  influence/NN\n",
      "  many/JJ\n",
      "  areas/NNS\n",
      "  life/NN\n",
      "  scores/NNS\n",
      "  tests/VBZ\n",
      "  college/NN\n",
      "  admissions/NNS\n",
      "  job/NN\n",
      "  opportunities/NNS\n",
      "  Reading/NNP\n",
      "  books/NNS\n",
      "  best/JJS\n",
      "  way/NN\n",
      "  enlarge/NN\n",
      "  exposure/VBP\n",
      "  new/JJ\n",
      "  words.Reading/NN\n",
      "  also/RB\n",
      "  releases/VBZ\n",
      "  stress/JJ\n",
      "  slows/VBZ\n",
      "  age-related/JJ\n",
      "  cognitive/JJ\n",
      "  decline/NN\n",
      "  cures/NNS\n",
      "  depression/NN\n",
      "  symptoms/NNS\n",
      "  therefore/RB\n",
      "  provides/VBZ\n",
      "  us/PRP\n",
      "  ample/JJ\n",
      "  benefits.Good/NN\n",
      "  books/NNS\n",
      "  also/RB\n",
      "  help/VBP\n",
      "  inspire/VB\n",
      "  people/NNS\n",
      "  write/VBP\n",
      "  (PERSON Research/NNP)\n",
      "  also/RB\n",
      "  proposes/VBZ\n",
      "  children/NNS\n",
      "  read/JJ\n",
      "  entertainment/NN\n",
      "  every/DT\n",
      "  day/NN\n",
      "  achieve/VB\n",
      "  reading/NN\n",
      "  tests/NNS\n",
      "  also/RB\n",
      "  exhibit/VBP\n",
      "  better/JJR\n",
      "  vocabulary/NN\n",
      "  increased/VBD\n",
      "  general/JJ\n",
      "  knowledge/NN\n",
      "  better/RBR\n",
      "  perception/NN\n",
      "  different/JJ\n",
      "  cultures.Also/NN\n",
      "  reading/NN\n",
      "  enjoyment/NN\n",
      "  likely/RB\n",
      "  decide/IN\n",
      "  whether/IN\n",
      "  child/NN\n",
      "  well/RB\n",
      "  school/NN\n",
      "  social/JJ\n",
      "  financial/JJ\n",
      "  background.If/NN\n",
      "  looking/VBG\n",
      "  custom/NN\n",
      "  essays/NNS\n",
      "  reading/VBG\n",
      "  topics/NNS\n",
      "  fastest/JJS\n",
      "  reliable/JJ\n",
      "  way/NN\n",
      "  buy/VB\n",
      "  essay/JJ\n",
      "  online/JJ\n",
      "  academic/JJ\n",
      "  writing/NN\n",
      "  experts/NNS)\n"
     ]
    }
   ],
   "source": [
    "print(ner1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "169d06db-b6aa-4cab-b53a-e5f67380aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d9722f3-c192-439e-9050-3000fe561731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reading</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>been</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>proven</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>online</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>academic</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>writing</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>experts</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word POS tag\n",
       "0     Reading      NN\n",
       "1         has     VBZ\n",
       "2        been     VBN\n",
       "3      proven     VBN\n",
       "4          to      TO\n",
       "..        ...     ...\n",
       "207    online      NN\n",
       "208      from      IN\n",
       "209  academic      JJ\n",
       "210   writing      NN\n",
       "211   experts     NNS\n",
       "\n",
       "[212 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_pos_tagged=nltk.pos_tag(sent.split())\n",
    "pd.DataFrame(nltk_pos_tagged,columns=['word','POS tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24a93434-73ee-4fca-a4e1-d93d05c1a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db14b07a-b37c-40c2-ac73-9bc29be7e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_to_be_extracted = r''' Chunk: {<DT>*<NNP>*<NN>*<VBG>} '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "226f7bba-8d19-4a9b-8e12-3deb679ca60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunk: {<DT>*<NNP>*<NN>*} \n"
     ]
    }
   ],
   "source": [
    "# print(chunk_to_be_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2dce240-43f0-458e-a600-430aa1f7ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r'NP:{<DT>*<VBZ>*<VBN>*<VBG>?<JJ>*<NNS>*<NNP>*<RB>*<NN>}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d2b4a90-04a6-4e15-b282-361eb6c81204",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "057097e4-57e4-4915-ab99-2dee3ae6b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkresult=chunk.parse(ner1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77241112-93bb-468b-af08-f0f5d8c694cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Reading/VBG\n",
      "  proven/RB\n",
      "  strengthen/VB\n",
      "  (NP brain/NN)\n",
      "  (NP Reading/NNP process/NN)\n",
      "  (NP brain/NN)\n",
      "  trained/VBD\n",
      "  keep/VB\n",
      "  (NP new/JJ information/NN)\n",
      "  new/JJ\n",
      "  topics/NNS\n",
      "  It/PRP\n",
      "  also/RB\n",
      "  helps/VBZ\n",
      "  writing/VBG\n",
      "  speaking/VBG\n",
      "  (ORGANIZATION fluentlyReading/NN)\n",
      "  also/RB\n",
      "  (NP proven/JJ increase/NN)\n",
      "  (NP empathy/JJ Literary/NNP fiction/NN)\n",
      "  —/NNP\n",
      "  stories/NNS\n",
      "  explore/VBP\n",
      "  inner/JJ\n",
      "  lives/NNS\n",
      "  characters/NNS\n",
      "  —/VBP\n",
      "  especially/RB\n",
      "  beneficial/JJ\n",
      "  growing/VBG\n",
      "  people/NNS\n",
      "  ’/NNP\n",
      "  empathy/JJ\n",
      "  towards/NNS\n",
      "  others.Reading/VBG\n",
      "  builds/NNS\n",
      "  (NP vocabulary/JJ important/JJ people/NNS Vocabulary/NNP size/NN)\n",
      "  (NP influence/NN)\n",
      "  (NP many/JJ areas/NNS life/NN)\n",
      "  scores/NNS\n",
      "  (NP tests/VBZ college/NN)\n",
      "  (NP admissions/NNS job/NN)\n",
      "  opportunities/NNS\n",
      "  Reading/NNP\n",
      "  books/NNS\n",
      "  best/JJS\n",
      "  (NP way/NN)\n",
      "  (NP enlarge/NN)\n",
      "  exposure/VBP\n",
      "  (NP new/JJ words.Reading/NN)\n",
      "  also/RB\n",
      "  releases/VBZ\n",
      "  stress/JJ\n",
      "  (NP slows/VBZ age-related/JJ cognitive/JJ decline/NN)\n",
      "  (NP cures/NNS depression/NN)\n",
      "  symptoms/NNS\n",
      "  therefore/RB\n",
      "  provides/VBZ\n",
      "  us/PRP\n",
      "  (NP ample/JJ benefits.Good/NN)\n",
      "  books/NNS\n",
      "  also/RB\n",
      "  help/VBP\n",
      "  inspire/VB\n",
      "  people/NNS\n",
      "  write/VBP\n",
      "  (PERSON Research/NNP)\n",
      "  also/RB\n",
      "  proposes/VBZ\n",
      "  children/NNS\n",
      "  (NP read/JJ entertainment/NN)\n",
      "  (NP every/DT day/NN)\n",
      "  achieve/VB\n",
      "  (NP reading/NN)\n",
      "  tests/NNS\n",
      "  also/RB\n",
      "  exhibit/VBP\n",
      "  better/JJR\n",
      "  (NP vocabulary/NN)\n",
      "  increased/VBD\n",
      "  (NP general/JJ knowledge/NN)\n",
      "  better/RBR\n",
      "  (NP perception/NN)\n",
      "  (NP different/JJ cultures.Also/NN)\n",
      "  (NP reading/NN)\n",
      "  (NP enjoyment/NN)\n",
      "  likely/RB\n",
      "  decide/IN\n",
      "  whether/IN\n",
      "  (NP child/NN)\n",
      "  (NP well/RB school/NN)\n",
      "  (NP social/JJ financial/JJ background.If/NN)\n",
      "  (NP looking/VBG custom/NN)\n",
      "  essays/NNS\n",
      "  reading/VBG\n",
      "  topics/NNS\n",
      "  fastest/JJS\n",
      "  (NP reliable/JJ way/NN)\n",
      "  buy/VB\n",
      "  (NP essay/JJ online/JJ academic/JJ writing/NN)\n",
      "  experts/NNS)\n"
     ]
    }
   ],
   "source": [
    "print(chunkresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a0b47-ed1d-48c9-8292-df93df4d10e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
