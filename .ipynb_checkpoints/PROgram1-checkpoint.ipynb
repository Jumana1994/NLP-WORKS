{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3765e763-a163-4cab-8859-a6999c209717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentense segmentation got skipped and word tokenizer is done\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b81430e2-4dc0-4a8a-8ce6-9d13ad62180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\" These sdrabats must be kidding. Among all three pacts The Pact of Treating All Sentient Beings Equally is just out of any sense. Humans are xenophobic but I do believe it’s a good thing; aliens never gave us nothing but wars. And now out of the blue Doroses disagree. They will make us collaborate with aliens, with robots of any kind, and the next thing you learn you are a slave. Why would we make diplomatic or trade relations with them? Humans are superior in any way, we need no aliens. No, now any aggressive action towards “sentient beings” may be seen as an act of treachery, and the offender would undergo criminal charges. That’s bad news, especially given such a vague definition of “sentient”. Should I ask a cow before slicing it whether it’s sentient? That would be hilarious for sure. It will not be enough though, since my cow doesn’t understand human language, but deep inside it can have clever thoughts of its own, which means intelligence. What are the criteria to determine if a creature is sentient? How are they even going to propagate such laws? How will it affect hunting? Will they issue binoculars that will tell you wether a game is sentient or not?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fe9618b-0cc1-441d-a606-40719aa413f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "420c26dd-0d49-465f-84df-3ca811ea2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ef73bd5-a1c9-4a1c-b452-52196c317fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3686445f-de0b-4614-a259-b330f0ea8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "228f0792-06fc-4a9d-913c-b8d36891e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d64e4c62-b5a7-4590-a5d3-39b034eabe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion  to check whether single token repeated how many times-freqDist\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "457c9514-c953-4f7a-8ebe-3847f484e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd707db8-54c0-4bca-a41f-4c0b7bb2609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 10, ',': 9, 'of': 7, 'sentient': 6, 'it': 6, 'a': 6, '?': 6, 'the': 5, 'are': 5, 'will': 5, ...})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting\n",
    "for i in token:\n",
    "    frequency[i.lower()]+=1\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53a7e8df-2d00-4ba7-8804-e927a22979c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count most repeated\n",
    "most_repeated=frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d464b886-6fdb-45d8-b798-510427984bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 10),\n",
       " (',', 9),\n",
       " ('of', 7),\n",
       " ('sentient', 6),\n",
       " ('it', 6),\n",
       " ('a', 6),\n",
       " ('?', 6),\n",
       " ('the', 5),\n",
       " ('are', 5),\n",
       " ('will', 5)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22ed75a4-c257-430a-aa81-c569fdf01d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization divided to- 3 bigram,:word split to 2 token ,trigram:3,ngram: split to n tokens\n",
    "# 2 words in token are joined to become one ,all words are done\n",
    "from nltk.util import bigrams,trigrams,ngrams\n",
    "bigram=list(bigrams(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d875dbbd-277a-4335-aa3d-0d3aee4ae733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff5a440c-0cc1-4483-9150-7a35d913ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram=list(trigrams(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6af0d6f8-b7ad-44c3-b279-919d45d02fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d7ef7a1-8643-40e2-8c16-c11fc4a31d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram=list(ngrams(token,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36e4b714-afe0-4022-9b2d-bc2966c0fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8c0e7cb-7c7c-4471-bf48-c729669222d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove punctuations(.,,.?,)\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce6ab580-da57-4818-85cc-07c19fd56e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e64100f3-d7d5-4ebd-812b-b856f9c16760",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in token:\n",
    "    if i not in string.punctuation:\n",
    "        a.append(i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1a18637-0984-477d-a95d-1b0f8b46a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40b110fe-6ca6-4d92-9e60-72faaba0d0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05a6a6ad-7477-4661-bc99-531885f77f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'of': 14, 'sentient': 12, 'it': 12, 'a': 12, '.': 10, 'the': 10, 'are': 10, 'will': 10, ',': 9, 'be': 8, ...})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in a:\n",
    "    frequency[i.lower()]+=1\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8421c-85b6-432f-a4c5-d6796de959c9",
   "metadata": {},
   "source": [
    "STEMMING DIVIded into 3\n",
    "porter stemmer\n",
    "Lancaster stemmer\n",
    "Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc0a719d-0b64-4712-9d4c-2860674c956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eef1c359-ef77-42f9-99f6-48b823b8766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0570e535-8ec8-4a13-a7fa-dcf14c2de507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b1ad508-d24d-4ed2-be99-a895e453afe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veri'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"very\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a00b1c3b-8674-4dbf-a6ad-9a3bdadd7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These - these\n",
      "sdrabats - sdrabat\n",
      "must - must\n",
      "be - be\n",
      "kidding - kid\n",
      ". - .\n",
      "Among - among\n",
      "all - all\n",
      "three - three\n",
      "pacts - pact\n",
      "The - the\n",
      "Pact - pact\n",
      "of - of\n",
      "Treating - treat\n",
      "All - all\n",
      "Sentient - sentient\n",
      "Beings - be\n",
      "Equally - equal\n",
      "is - is\n",
      "just - just\n",
      "out - out\n",
      "of - of\n",
      "any - ani\n",
      "sense - sens\n",
      ". - .\n",
      "Humans - human\n",
      "are - are\n",
      "xenophobic - xenophob\n",
      "but - but\n",
      "I - i\n",
      "do - do\n",
      "believe - believ\n",
      "it - it\n",
      "’ - ’\n",
      "s - s\n",
      "a - a\n",
      "good - good\n",
      "thing - thing\n",
      "; - ;\n",
      "aliens - alien\n",
      "never - never\n",
      "gave - gave\n",
      "us - us\n",
      "nothing - noth\n",
      "but - but\n",
      "wars - war\n",
      ". - .\n",
      "And - and\n",
      "now - now\n",
      "out - out\n",
      "of - of\n",
      "the - the\n",
      "blue - blue\n",
      "Doroses - doros\n",
      "disagree - disagre\n",
      ". - .\n",
      "They - they\n",
      "will - will\n",
      "make - make\n",
      "us - us\n",
      "collaborate - collabor\n",
      "with - with\n",
      "aliens - alien\n",
      ", - ,\n",
      "with - with\n",
      "robots - robot\n",
      "of - of\n",
      "any - ani\n",
      "kind - kind\n",
      ", - ,\n",
      "and - and\n",
      "the - the\n",
      "next - next\n",
      "thing - thing\n",
      "you - you\n",
      "learn - learn\n",
      "you - you\n",
      "are - are\n",
      "a - a\n",
      "slave - slave\n",
      ". - .\n",
      "Why - whi\n",
      "would - would\n",
      "we - we\n",
      "make - make\n",
      "diplomatic - diplomat\n",
      "or - or\n",
      "trade - trade\n",
      "relations - relat\n",
      "with - with\n",
      "them - them\n",
      "? - ?\n",
      "Humans - human\n",
      "are - are\n",
      "superior - superior\n",
      "in - in\n",
      "any - ani\n",
      "way - way\n",
      ", - ,\n",
      "we - we\n",
      "need - need\n",
      "no - no\n",
      "aliens - alien\n",
      ". - .\n",
      "No - no\n",
      ", - ,\n",
      "now - now\n",
      "any - ani\n",
      "aggressive - aggress\n",
      "action - action\n",
      "towards - toward\n",
      "“ - “\n",
      "sentient - sentient\n",
      "beings - be\n",
      "” - ”\n",
      "may - may\n",
      "be - be\n",
      "seen - seen\n",
      "as - as\n",
      "an - an\n",
      "act - act\n",
      "of - of\n",
      "treachery - treacheri\n",
      ", - ,\n",
      "and - and\n",
      "the - the\n",
      "offender - offend\n",
      "would - would\n",
      "undergo - undergo\n",
      "criminal - crimin\n",
      "charges - charg\n",
      ". - .\n",
      "That - that\n",
      "’ - ’\n",
      "s - s\n",
      "bad - bad\n",
      "news - news\n",
      ", - ,\n",
      "especially - especi\n",
      "given - given\n",
      "such - such\n",
      "a - a\n",
      "vague - vagu\n",
      "definition - definit\n",
      "of - of\n",
      "“ - “\n",
      "sentient - sentient\n",
      "” - ”\n",
      ". - .\n",
      "Should - should\n",
      "I - i\n",
      "ask - ask\n",
      "a - a\n",
      "cow - cow\n",
      "before - befor\n",
      "slicing - slice\n",
      "it - it\n",
      "whether - whether\n",
      "it - it\n",
      "’ - ’\n",
      "s - s\n",
      "sentient - sentient\n",
      "? - ?\n",
      "That - that\n",
      "would - would\n",
      "be - be\n",
      "hilarious - hilari\n",
      "for - for\n",
      "sure - sure\n",
      ". - .\n",
      "It - it\n",
      "will - will\n",
      "not - not\n",
      "be - be\n",
      "enough - enough\n",
      "though - though\n",
      ", - ,\n",
      "since - sinc\n",
      "my - my\n",
      "cow - cow\n",
      "doesn - doesn\n",
      "’ - ’\n",
      "t - t\n",
      "understand - understand\n",
      "human - human\n",
      "language - languag\n",
      ", - ,\n",
      "but - but\n",
      "deep - deep\n",
      "inside - insid\n",
      "it - it\n",
      "can - can\n",
      "have - have\n",
      "clever - clever\n",
      "thoughts - thought\n",
      "of - of\n",
      "its - it\n",
      "own - own\n",
      ", - ,\n",
      "which - which\n",
      "means - mean\n",
      "intelligence - intellig\n",
      ". - .\n",
      "What - what\n",
      "are - are\n",
      "the - the\n",
      "criteria - criteria\n",
      "to - to\n",
      "determine - determin\n",
      "if - if\n",
      "a - a\n",
      "creature - creatur\n",
      "is - is\n",
      "sentient - sentient\n",
      "? - ?\n",
      "How - how\n",
      "are - are\n",
      "they - they\n",
      "even - even\n",
      "going - go\n",
      "to - to\n",
      "propagate - propag\n",
      "such - such\n",
      "laws - law\n",
      "? - ?\n",
      "How - how\n",
      "will - will\n",
      "it - it\n",
      "affect - affect\n",
      "hunting - hunt\n",
      "? - ?\n",
      "Will - will\n",
      "they - they\n",
      "issue - issu\n",
      "binoculars - binocular\n",
      "that - that\n",
      "will - will\n",
      "tell - tell\n",
      "you - you\n",
      "wether - wether\n",
      "a - a\n",
      "game - game\n",
      "is - is\n",
      "sentient - sentient\n",
      "or - or\n",
      "not - not\n",
      "? - ?\n"
     ]
    }
   ],
   "source": [
    "for i in token:\n",
    "    print(i,'-',stemmer.stem(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a24ecea-3f6d-4aa8-b829-43c06b19178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lanstemmer=LancasterStemmer()\n",
    "lanstemmer.stem(\"very\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4cafd3a9-a932-4420-982e-90f26dc22a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in token:\n",
    "#     print(i,'-',lanstemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fbe59b46-bb64-4b91-96ed-9360d767b234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veri'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowstemmer=SnowballStemmer('english')\n",
    "snowstemmer.stem(\"very\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7fbcc946-2e02-45ce-adb0-55d9eddf9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in token:\n",
    "#     print(i,'-',snowstemmer.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171a9fd-5ffd-43dd-8a9b-b81f5df545bb",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "53368d20-5df1-40c1-8a75-f093d4d58723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f291d823-dd94-45ab-a71d-ccea029eb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f99dea2-df57-4028-b81e-34c4858484de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These These\n",
      "sdrabats sdrabats\n",
      "must must\n",
      "be be\n",
      "kidding kidding\n",
      ". .\n",
      "Among Among\n",
      "all all\n",
      "three three\n",
      "pacts pact\n",
      "The The\n",
      "Pact Pact\n",
      "of of\n",
      "Treating Treating\n",
      "All All\n",
      "Sentient Sentient\n",
      "Beings Beings\n",
      "Equally Equally\n",
      "is is\n",
      "just just\n",
      "out out\n",
      "of of\n",
      "any any\n",
      "sense sense\n",
      ". .\n",
      "Humans Humans\n",
      "are are\n",
      "xenophobic xenophobic\n",
      "but but\n",
      "I I\n",
      "do do\n",
      "believe believe\n",
      "it it\n",
      "’ ’\n",
      "s s\n",
      "a a\n",
      "good good\n",
      "thing thing\n",
      "; ;\n",
      "aliens alien\n",
      "never never\n",
      "gave gave\n",
      "us u\n",
      "nothing nothing\n",
      "but but\n",
      "wars war\n",
      ". .\n",
      "And And\n",
      "now now\n",
      "out out\n",
      "of of\n",
      "the the\n",
      "blue blue\n",
      "Doroses Doroses\n",
      "disagree disagree\n",
      ". .\n",
      "They They\n",
      "will will\n",
      "make make\n",
      "us u\n",
      "collaborate collaborate\n",
      "with with\n",
      "aliens alien\n",
      ", ,\n",
      "with with\n",
      "robots robot\n",
      "of of\n",
      "any any\n",
      "kind kind\n",
      ", ,\n",
      "and and\n",
      "the the\n",
      "next next\n",
      "thing thing\n",
      "you you\n",
      "learn learn\n",
      "you you\n",
      "are are\n",
      "a a\n",
      "slave slave\n",
      ". .\n",
      "Why Why\n",
      "would would\n",
      "we we\n",
      "make make\n",
      "diplomatic diplomatic\n",
      "or or\n",
      "trade trade\n",
      "relations relation\n",
      "with with\n",
      "them them\n",
      "? ?\n",
      "Humans Humans\n",
      "are are\n",
      "superior superior\n",
      "in in\n",
      "any any\n",
      "way way\n",
      ", ,\n",
      "we we\n",
      "need need\n",
      "no no\n",
      "aliens alien\n",
      ". .\n",
      "No No\n",
      ", ,\n",
      "now now\n",
      "any any\n",
      "aggressive aggressive\n",
      "action action\n",
      "towards towards\n",
      "“ “\n",
      "sentient sentient\n",
      "beings being\n",
      "” ”\n",
      "may may\n",
      "be be\n",
      "seen seen\n",
      "as a\n",
      "an an\n",
      "act act\n",
      "of of\n",
      "treachery treachery\n",
      ", ,\n",
      "and and\n",
      "the the\n",
      "offender offender\n",
      "would would\n",
      "undergo undergo\n",
      "criminal criminal\n",
      "charges charge\n",
      ". .\n",
      "That That\n",
      "’ ’\n",
      "s s\n",
      "bad bad\n",
      "news news\n",
      ", ,\n",
      "especially especially\n",
      "given given\n",
      "such such\n",
      "a a\n",
      "vague vague\n",
      "definition definition\n",
      "of of\n",
      "“ “\n",
      "sentient sentient\n",
      "” ”\n",
      ". .\n",
      "Should Should\n",
      "I I\n",
      "ask ask\n",
      "a a\n",
      "cow cow\n",
      "before before\n",
      "slicing slicing\n",
      "it it\n",
      "whether whether\n",
      "it it\n",
      "’ ’\n",
      "s s\n",
      "sentient sentient\n",
      "? ?\n",
      "That That\n",
      "would would\n",
      "be be\n",
      "hilarious hilarious\n",
      "for for\n",
      "sure sure\n",
      ". .\n",
      "It It\n",
      "will will\n",
      "not not\n",
      "be be\n",
      "enough enough\n",
      "though though\n",
      ", ,\n",
      "since since\n",
      "my my\n",
      "cow cow\n",
      "doesn doesn\n",
      "’ ’\n",
      "t t\n",
      "understand understand\n",
      "human human\n",
      "language language\n",
      ", ,\n",
      "but but\n",
      "deep deep\n",
      "inside inside\n",
      "it it\n",
      "can can\n",
      "have have\n",
      "clever clever\n",
      "thoughts thought\n",
      "of of\n",
      "its it\n",
      "own own\n",
      ", ,\n",
      "which which\n",
      "means mean\n",
      "intelligence intelligence\n",
      ". .\n",
      "What What\n",
      "are are\n",
      "the the\n",
      "criteria criterion\n",
      "to to\n",
      "determine determine\n",
      "if if\n",
      "a a\n",
      "creature creature\n",
      "is is\n",
      "sentient sentient\n",
      "? ?\n",
      "How How\n",
      "are are\n",
      "they they\n",
      "even even\n",
      "going going\n",
      "to to\n",
      "propagate propagate\n",
      "such such\n",
      "laws law\n",
      "? ?\n",
      "How How\n",
      "will will\n",
      "it it\n",
      "affect affect\n",
      "hunting hunting\n",
      "? ?\n",
      "Will Will\n",
      "they they\n",
      "issue issue\n",
      "binoculars binoculars\n",
      "that that\n",
      "will will\n",
      "tell tell\n",
      "you you\n",
      "wether wether\n",
      "a a\n",
      "game game\n",
      "is is\n",
      "sentient sentient\n",
      "or or\n",
      "not not\n",
      "? ?\n"
     ]
    }
   ],
   "source": [
    "for i in token:\n",
    "    print(i,lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9701a5-ae64-4ef0-bb7e-6841fc0df1f7",
   "metadata": {},
   "source": [
    "REMOVING STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3e0e36cc-5c84-48b4-98cf-0616f8aa7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac74e601-1e29-4186-b2bd-794dbbe7afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69b4a146-ecfe-4d36-856a-fbaf99354d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "37fdbe21-27fc-4ace-94d9-e2f5b2b1df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in token:\n",
    "    if i not in stop_words:\n",
    "        b.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b38e144-360e-48fb-92c6-d57c98b5330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5199393a-db46-43a6-aa37-648a3acfca55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec7cd5c8-07bf-42e5-93e5-5aeac9120f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8c229ff-8c79-418c-9d1a-57b2c1004d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599df174-202d-4195-865a-4e32796d7288",
   "metadata": {},
   "source": [
    "pos-parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "658cb9c7-ff78-47bb-99d1-19b7e1c2acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"my name is Jumana my son AzimMohammed and my daughter Aishahyrin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1911ee0e-a187-46d7-87ce-630752d18c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Jumana',\n",
       " 'my',\n",
       " 'son',\n",
       " 'AzimMohammed',\n",
       " 'and',\n",
       " 'my',\n",
       " 'daughter',\n",
       " 'Aishahyrin']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentoken=word_tokenize(sent)\n",
    "sentoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "882ff5d3-865c-4d6c-abb2-69d8190becf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_word1=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30a7bff8-b636-4a78-bfd6-707d038cc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in sentoken:\n",
    "    if i not in stop_word1:\n",
    "        b.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "312e633f-eb32-489b-b120-66954f436bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'Jumana', 'son', 'AzimMohammed', 'daughter', 'Aishahyrin']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ed2fa-a954-4054-86ec-7f4e55418ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9a1ae28c-4579-4304-ab11-9a41f61824c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'NN'),\n",
       " ('Jumana', 'NNP'),\n",
       " ('son', 'NN'),\n",
       " ('AzimMohammed', 'NNP'),\n",
       " ('daughter', 'NN'),\n",
       " ('Aishahyrin', 'NNP')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_string=nltk.pos_tag(b)\n",
    "pos_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129c92f-202b-4f22-b728-b3470feb9838",
   "metadata": {},
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6cb20bd-51b2-40a6-b32a-55186d9eab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e104e5b1-8fca-4f88-8b21-adba301b7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a8d9a645-cfae-4480-b094-69d321141ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner=ne_chunk(pos_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "48e6a4fd-7415-46f2-a293-d31ffcd93ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  name/NN\n",
      "  (PERSON Jumana/NNP)\n",
      "  son/NN\n",
      "  (ORGANIZATION AzimMohammed/NNP)\n",
      "  daughter/NN\n",
      "  (PERSON Aishahyrin/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d367a2b-df1a-46ff-82cb-334548ac1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammer defining,chunking\n",
    "# grammar_np=r'NP:{<DT>?<JJ>*<NN>}'    #grammer defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2750f760-c652-4a6e-ba99-ccf94915e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r'NP:{<NN>}'    #grammer defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "40d26d8a-fb74-4ac2-a269-58f1edff8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=nltk.RegexpParser(grammar_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "16f83a2d-f73b-4306-ad69-dd6f9b8cf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkresult=chunk.parse(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f96721d0-15d3-4bb4-97f5-09a4df915ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP name/NN)\n",
      "  (PERSON Jumana/NNP)\n",
      "  (NP son/NN)\n",
      "  (ORGANIZATION AzimMohammed/NNP)\n",
      "  (NP daughter/NN)\n",
      "  (PERSON Aishahyrin/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(chunkresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "66709cb9-f165-4252-b1db-a4c442d618d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b='The big fish eats small fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e84e934-b456-4f01-ab73-a116cecab566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d8c39061-26c5-4876-952f-27f58106eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=word_tokenize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ea45c10-b56e-41cc-b448-657523a18faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'big', 'fish', 'eats', 'small', 'fish']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ff22aa0-9c34-4e9d-92d4-d897d4b6081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('fish', 'NN'),\n",
       " ('eats', 'NNS'),\n",
       " ('small', 'JJ'),\n",
       " ('fish', 'NN')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_string1=nltk.pos_tag(word)\n",
    "pos_string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab414fc1-ee00-455e-a76d-b3cb2751f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner1=ne_chunk(pos_string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bf0917be-e9a5-4748-9d4e-b26244b85e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S The/DT big/JJ fish/NN eats/NNS small/JJ fish/NN)\n"
     ]
    }
   ],
   "source": [
    "print(ner1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b27ecd6d-4d1d-41ac-9b2a-7c86144084ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r'NP:{<DT>?<JJ>*<NN>}'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "79109b9b-78cf-45e6-80f7-2956c59e60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1=nltk.RegexpParser(grammar_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f2ff3304-5d0d-40e3-8d3e-c75b3a66a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkresult1=chunk.parse(ner1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a2ec88fc-8f0c-41a2-aca6-c6e1aec12f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S The/DT big/JJ (NP fish/NN) eats/NNS small/JJ (NP fish/NN))\n"
     ]
    }
   ],
   "source": [
    "print(chunkresult1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b2b5d-59b6-40f8-8f96-b403095f00bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
